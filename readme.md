# **Data Engineering Pipeline â€“ Sales & Customer Analytics**

## **ğŸ“Œ Project Overview**

This project implements an **end-to-end ETL pipeline** using **PySpark**, **AWS S3**, and **MySQL** for processing sales and customer data. It validates incoming CSV files, enriches them with dimension tables, calculates key business metrics, and generates data marts for analytical use.

---

## **ğŸ› ï¸ Key Features**

âœ” **Automated CSV Ingestion & Validation**  
âœ” **Schema Validation (Mandatory Columns Check)**  
âœ” **Error Handling & Rejected Files Processing (moved to `rejected_csv` in S3)**  
âœ” **Data Enrichment using Dimension Tables**  
âœ” **Customer & Sales Team Data Marts Generation**  
âœ” **Partitioned Parquet Storage for Analytical Workloads**  
âœ” **Automatic S3 File Movement (Raw â†’ Processed)**  
âœ” **Incremental Status Updates in MySQL Staging Table**

---

## **ğŸ“‚ Project Workflow**

### **1. Input Validation**
- Reads CSV files from **AWS S3 (raw source directory)**  
- Checks mandatory columns against config  
- Moves incorrect files to `rejected_csv/` in S3

### **2. Staging Table Updates**
- Inserts validated files into **MySQL staging table** with status `'A'` (active)  
- Updates processed files to status `'I'` (inactive)

### **3. Data Transformation**
- Joins fact data with dimension tables (Customer, Store, Sales Team)  
- Handles extra columns by consolidating them into an `additional_column`

### **4. Data Marts**
- **Customer Data Mart:** Monthly purchase summary for customers  
- **Sales Team Data Mart:** Incentive and sales performance calculations  
- Stores results locally as Parquet â†’ uploads to S3

### **5. Partitioned Data for Analytics**
- Saves **partitioned parquet** by `sales_month` & `store_id`  
- Uploads partitioned data to S3 with timestamped folder structure

### **6. Calculations**
- Monthly amount purchased per customer  
- Incentive calculations for sales team

---

## **âš™ï¸ Technologies Used**

- **Python 3.x**
- **PySpark (ETL, Transformations)**
- **AWS S3 (Raw, Rejected, Processed, Data Marts)**
- **MySQL (Staging & Dimension Tables)**
- **Boto3 (AWS S3 operations)**

---

Requirement to complete the projects:-
1. You should have laptop with minimum 4 GB of RAM, i3 and above (Better to have 8GB with i5).
2. PyCharm installed in the system.
4. MySQL workbench should also be installed to the system.
5. GitHub account is good to have but not necessary.
5. You should have AWS account.
6. Understanding of spark,sql and python is required.

```plaintext
Project structure:-
my_project/
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ readme.md
â”œâ”€â”€ resources/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚    â”œâ”€â”€ config.py
â”‚   â”‚    â””â”€â”€ requirement.txt
â”‚   â””â”€â”€ qa/
â”‚   â”‚    â”œâ”€â”€ config.py
â”‚   â”‚    â””â”€â”€ requirement.txt
â”‚   â””â”€â”€ prod/
â”‚   â”‚    â”œâ”€â”€ config.py
â”‚   â”‚    â””â”€â”€ requirement.txt
â”‚   â”œâ”€â”€ sql_scripts/
â”‚   â”‚    â””â”€â”€ table_scripts.sql
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚    â”œâ”€â”€ __init__.py
â”‚   â”‚    â””â”€â”€ delete/
â”‚   â”‚    â”‚      â”œâ”€â”€ aws_delete.py
â”‚   â”‚    â”‚      â”œâ”€â”€ database_delete.py
â”‚   â”‚    â”‚      â””â”€â”€ local_file_delete.py
â”‚   â”‚    â””â”€â”€ download/
â”‚   â”‚    â”‚      â””â”€â”€ aws_file_download.py
â”‚   â”‚    â””â”€â”€ move/
â”‚   â”‚    â”‚      â””â”€â”€ move_files.py
â”‚   â”‚    â””â”€â”€ read/
â”‚   â”‚    â”‚      â”œâ”€â”€ aws_read.py
â”‚   â”‚    â”‚      â””â”€â”€ database_read.py
â”‚   â”‚    â””â”€â”€ transformations/
â”‚   â”‚    â”‚      â””â”€â”€ jobs/
â”‚   â”‚    â”‚      â”‚     â”œâ”€â”€ customer_mart_sql_transform_write.py
â”‚   â”‚    â”‚      â”‚     â”œâ”€â”€ dimension_tables_join.py
â”‚   â”‚    â”‚      â”‚     â”œâ”€â”€ main.py
â”‚   â”‚    â”‚      â”‚     â””â”€â”€sales_mart_sql_transform_write.py
â”‚   â”‚    â””â”€â”€ upload/
â”‚   â”‚    â”‚      â””â”€â”€ upload_to_s3.py
â”‚   â”‚    â””â”€â”€ utility/
â”‚   â”‚    â”‚      â”œâ”€â”€ encrypt_decrypt.py
â”‚   â”‚    â”‚      â”œâ”€â”€ logging_config.py
â”‚   â”‚    â”‚      â”œâ”€â”€ s3_client_object.py
â”‚   â”‚    â”‚      â”œâ”€â”€ spark_session.py
â”‚   â”‚    â”‚      â””â”€â”€ my_sql_session.py
â”‚   â”‚    â””â”€â”€ write/
â”‚   â”‚    â”‚      â”œâ”€â”€ database_write.py
â”‚   â”‚    â”‚      â””â”€â”€ parquet_write.py
â”‚   â”œâ”€â”€ test/
â”‚   â”‚    â”œâ”€â”€ scratch_pad.py.py
â”‚   â”‚    â””â”€â”€ generate_csv_data.py
```

How to run the program in Pycharm:-
1. Open the pycharm editor.
2. Upload or pull the project from GitHub.
3. Open terminal from bottom pane.
4. Goto virtual environment and activate it. Let's say you have venv as virtual environament.i) cd venv ii) cd Scripts iii) activate (if activate doesn't work then use ./activate)
5. Create main.py as explained in my videos on YouTube channel.
6. You will have to create a user on AWS also and assign s3 full access and provide secret key and access key to the config file.
6. Run main.py from green play button on top right hand side.
7. If everything works as expected enjoy, else re-try.

